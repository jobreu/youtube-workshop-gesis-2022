<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Automatic Sampling and Analysis of YouTube Data</title>
    <meta charset="utf-8" />
    <meta name="author" content="Julian Kohne Johannes Breuer M. Rohangis Mohseni" />
    <meta name="date" content="2022-02-21" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/default-fonts.css" rel="stylesheet" />
    <link href="libs/tile-view/tile-view.css" rel="stylesheet" />
    <script src="libs/tile-view/tile-view.js"></script>
    <script src="libs/clipboard/clipboard.min.js"></script>
    <link href="libs/xaringanExtra-clipboard/xaringanExtra-clipboard.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-clipboard/xaringanExtra-clipboard.js"></script>
    <script>window.xaringanExtraClipboard(null, {"button":"Copy Code","success":"Copied!","error":"Press Ctrl+C to Copy"})</script>
    <link href="libs/xaringanExtra-extra-styles/xaringanExtra-extra-styles.css" rel="stylesheet" />
    <link rel="stylesheet" href="workshop.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Automatic Sampling and Analysis of YouTube Data
## Processing and Cleaning User Comments
### Julian Kohne<br />Johannes Breuer<br />M. Rohangis Mohseni
### 2022-02-21

---


layout: true



&lt;div class="my-footer"&gt;
  &lt;div style="float: left;"&gt;&lt;span&gt;Julian Kohne, Johannes Breuer, M. Rohangis Mohseni&lt;/span&gt;&lt;/div&gt;
  &lt;div style="float: right;"&gt;&lt;span&gt;GESIS, online, 2022-02-21&lt;/span&gt;&lt;/div&gt;
  &lt;div style="text-align: center;"&gt;&lt;span&gt;Processing and Cleaning User Comments&lt;/span&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;style type="text/css"&gt;

pre {
  font-size: 10px
}
&lt;/style&gt;

---
class: center, middle
# Processing and Cleaning User Comments

---
# Preprocessing

- Preprocessing refers to all steps that need to be taken to make the data suitable for the actual analysis

- For webscraping data, this is often more tedious and time-consuming than for survey data because:
  - the data are not designed with your analysis in mind
  - the data are typically less structured
  - the data are typically more complex
  - the data are typically more heterogenous
  - the data are typically larger
  
- In addition, with large amounts of data it is often necessary to work on servers or clusters instead of regular desktop or laptop computers

- Even then, restructuring or transforming data can take days, so mistakes hurt more

---
# Preprocessing

- In _Data Science_, most time is typically spent on the preprocessing rather than the actual analysis

![plot](./Images/DS2.jpg)

.mini[Source: https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/#157890a96f63]

---
# Preprocessing

- Also, it is perceived as the least enjoyable part of the process

![plot](./Images/DS1.jpg)

.mini[Source: https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/#157890a96f63]
---
# Preprocessing _YouTube_ comments


- The `tuber` package returns an `R` dataframe instead of a JSON

- We can select which data we need by using the API through `tuber`

- For single videos, the data are small enough to be processed on a regular desktop/laptop computer

- However, this doesn't mean that the data are already usable for all intents and purposes

- We still need to:
  - select
  - format
  - extract
  - link
  
the information that is relevant to us

---
# Preprocessing _YouTube_ Comments

For this section, we are using comments from the Emoji Movie Trailer &lt;br&gt;
(https://www.youtube.com/watch?v=r8pJt4dK_s4)

![plot](./Images/EmojiMovie.png)
---
# Understanding Your Data (1)

The first step is always to understand your data, this is especially crucial for _found data_ because
it was not designed with your analysis in mind


```r
# loading raw data
comments &lt;- readRDS("../../data/RawEmojiComments.rds")

# listing all columns
colnames(comments)
```

```
##  [1] "videoId"               "textDisplay"           "textOriginal"         
##  [4] "authorDisplayName"     "authorProfileImageUrl" "authorChannelUrl"     
##  [7] "authorChannelId.value" "canRate"               "viewerRating"         
## [10] "likeCount"             "publishedAt"           "updatedAt"            
## [13] "id"                    "parentId"              "moderationStatus"
```

Luckily, the _YouTube_ API is very [well documented](https://developers.google.com/youtube/v3/docs/comments) and provides brief explanations for all the variables you can extract from it

---
# Understanding Your Data (2)

This information is valuable for understanding what type of comments the dataframe contains


```r
table(is.na(comments$parentId))
```

```
## 
## FALSE  TRUE 
## 15734 22600
```

A quick look at the documentation reveals:

**parentID**: _The unique ID of the parent comment. This property is only set if the comment was submitted as a reply to another comment._

---
# Understanding Your Data (3)

...or for knowing how specific data types are formatted


```r
head(comments$publishedAt)
```

```
## [1] "2022-02-10T06:38:33Z" "2022-02-08T04:05:05Z" "2022-02-06T16:43:18Z"
## [4] "2022-02-06T12:42:39Z" "2022-02-06T01:10:24Z" "2022-02-05T23:23:26Z"
```

```r
class(comments$publishedAt)
```

```
## [1] "character"
```

A quick look at the documentation reveals:

**publishedAt**: _The date and time when the comment was originally published. The value is specified in ISO 8601 (YYYY-MM-DDThh:mm:ss.sZ) format._

---
# Understanding Your Data (4)

...or how similar variables are different from each other


```r
comments$textOriginal[6]
```

```
## [1] "The best part 2:38"
```


```r
comments$textDisplay[6]
```


```
## [1] "The best part &lt;a href=\"https://www.youtube.com/watch?"
## [2] "v=r8pJt4dK_s4&amp;amp;t=2m38s\"&gt;2:38&lt;/a&gt;"
```

**textOriginal**: _The original, raw text of the comment as it was initially posted or last updated. The original text is only returned if it is accessible to the authenticated user, which is only guaranteed if the user is the comment's author._

**textDisplay**: _The comment's text. The text can be retrieved in either plain text or HTML. (The comments.list and commentThreads.list methods both support a textFormat parameter, which specifies the desired text format). Note that even the plain text may differ from the original comment text. For example, it may replace video links with video titles._

---
# Selecting What You (Don't) need

Now we can decide on what we need for our analyses


```r
Selection &lt;- subset(comments,select = -c(authorProfileImageUrl,
                                         authorChannelUrl,
                                         authorChannelId.value,
                                         videoId,
                                         canRate,
                                         viewerRating,
                                         moderationStatus))
colnames(Selection)
```

```
## [1] "textDisplay"       "textOriginal"      "authorDisplayName"
## [4] "likeCount"         "publishedAt"       "updatedAt"        
## [7] "id"                "parentId"
```

**Word of advice**: Always keep an unaltered copy of your raw data and don't overwrite it. You never know what kinds of mistakes/oversights you might notice down the line and you don't want to have to recollect everything. Save your parsed data in a separate file (or in multiple steps if your preprocessing pipeline is complex).

---
# Formatting your Data

By default, the data you get out of `tuber` is most likely not in the right format for your analyses


```r
sapply(Selection, class)
```

```
##       textDisplay      textOriginal authorDisplayName         likeCount 
##       "character"       "character"       "character"       "character" 
##       publishedAt         updatedAt                id          parentId 
##       "character"       "character"       "character"       "character"
```


```r
# Summary statistics for like counts
summary(Selection$likeCount)
```

```
##    Length     Class      Mode 
##     38334 character character
```


```r
# time difference between first comment and now
Sys.time() - Selection$publishedAt[1]
```

```
## Error in unclass(e1) - e2: non-numeric argument to binary operator
```

---
# Formatting `likeCount`

We want the `likeCount` to be a numeric variable and the timestamps to be datetime objects


```r
# Transforming likeCount to numeric
# (NB: this overwrites the original column)
Selection$likeCount &lt;- as.numeric(Selection$likeCount)

# testing
summary(Selection$likeCount)
```

```
##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
##    0.000    0.000    2.000    4.829    5.000 4344.000
```

We can now work with the number of likes as a numeric variable

---
# Formatting your Timestamps (1)

Timestamps are extremely complex objects due to:
 - Different calendars
 - Different formattings
 - Different origins
 - Different time zones
 - Historical anomalies
 - Different resolutions
 - Summer vs. Wintertime (different for each country and depending on hemisphere!)
 - Leap years
 - [etc.](https://www.youtube.com/watch?v=-5wpm-gesOY)
 
For these reasons, **never** try to code your own time stamp translations from scratch. Fortunately, `R` has several build in methods for dealing with this madness. The most basic one is the `as.POSIXct()` function, the most convenient one is the `anytime()` function from the `anytime` package (another powerful option for dealing with times and dates in `R` is the [`lubridate` package](https://lubridate.tidyverse.org/) from the `Tidyverse`).

---
# Formatting Timestamps (2)


```r
# transforming timestamps to datetime objects
Selection$publishedAt[1]
```

```
## [1] "2022-02-10T06:38:33Z"
```

```r
testtime &lt;- as.POSIXct(Selection$publishedAt[1],
                       format = "%Y-%m-%dT%H:%M:%OSZ",
                       tz = "UTC")
testtime
```

```
## [1] "2022-02-10 06:38:33 UTC"
```

```r
# testing whether we can compute a difference
# with the datetime object
Sys.time() - testtime
```

```
## Time difference of 5.323463 days
```

This internal representation of time objects will be extremely important for plotting trends over time
and calculating time differences. You can find an overview of formatting strings [here](https://rdrr.io/r/base/strptime.html).

---
# Formatting Timestamps (3)

A more convenient way to transform datetimes is the [`anytime` package](https://github.com/eddelbuettel/anytime). It automatically tries
to guess the format from the character string, so you don't have to. This is especially handy for vectors 
of datetimes in multiple different formats.


```r
# transforming datetimes using anytime()
library(anytime)
Selection$publishedAt &lt;- anytime(Selection$publishedAt,
                                 asUTC = TRUE)
Selection$updatedAt &lt;- anytime(Selection$updatedAt,
                               asUTC = TRUE)
sapply(list(Selection$publishedAt,Selection$updatedAt),class)
```

```
##      [,1]      [,2]     
## [1,] "POSIXct" "POSIXct"
## [2,] "POSIXt"  "POSIXt"
```

**Word of Advice**: For datetime conversions, always do some sanity checks, especially if you are using methods that automatically detect the format. Pay special attention to the _timezone_ in which your data are saved and compare it to the documentation of the standard.

---
# Formatting Timestamps (4)

Be aware of how to interpret your timestamps. Note that the date was interpreted as UTC but converted to our local CET timezone which is 1 hour ahead of UTC. This comment was made at 07:38:33 in _our time_, we have no idea about the time at the location of the user.


```r
Selection$publishedAt[1]
```

```
## [1] "2022-02-10 07:38:33 CET"
```
 
---
# Extracting Information

After having formatted all our selected columns, we usually also want to create some new columns with information that is not directly available in the raw data. For example, consider these comments:


```r
# Example comments with extractable information
strwrap(Selection$textOriginal[37445],79)
```

```
## [1] "Watch new Emoji movie [2017] Here: New Emoji movie 2017"
## [2] "https://www.clorox.com/"
```

```r
Selection$textOriginal[26]
```

```
## [1] "Here him üôÑüôÑüôÑüôÑüôÑüôÑ"
```
There are two issues exemplified by these comments:

1) Comments contain emoji and hyperlinks that might distort our text analysis later

2) These are features that we'd like to have in a separate column for our analysis

---
# Extracting Hyperlinks (1)

We will start with deleting hyperlinks from our text and saving them in an additional column. We will use the
text mining package [`qdapRegex`](https://github.com/trinker/qdapRegex) for this as it has predefined routines for handling large text vectors and [regular expressions](https://en.wikipedia.org/wiki/Regular_expression).


```r
# Note that we are using the original text so we don't have
#to deal with the HTML formatting of the links
library(qdapRegex)
Links &lt;- rm_url(Selection$textOriginal, extract = TRUE)
LinkDel &lt;- rm_url(Selection$textOriginal)
head(Links[!is.na(Links)],3)
```

```
## [[1]]
## [1] "https://youtu.be/59Tr9NDr5N4\nI"
## 
## [[2]]
## [1] "https://youtu.be/SgX3ggJv1Rw"
## 
## [[3]]
## [1] "https://m.youtube.com/watch?v=BLUkgRAy_Vo"
```

---
# Extracting Hyperlinks (2)

We get back a list where each element corresponds to one row in the Selection dataframe and contains a vector of
links that were contained in the textOriginal column. At the same time, the link was removed from the Selection dataframe.


```r
strwrap(Selection$textOriginal[37445],79)
```

```
## [1] "Watch new Emoji movie [2017] Here: New Emoji movie 2017"
## [2] "https://www.clorox.com/"
```


```r
LinkDel[37445]
```

```
## [1] "Watch new Emoji movie [2017] Here: New Emoji movie 2017"
```

```r
Links[[37445]]
```

```
## [1] "https://www.clorox.com/"
```

---
# Extracting Emoji (1)

The `qdapRegex` package has a lot of other different predefined functions for extracting or removing certain kinds of strings:
  - `rm_citation()`
  - `rm_date()`
  - `rm_phone()`
  - `rm_postal_code()`
  - `rm_email()`
  - `rm_dollar()`
  - `rm_emoticon()`
  
Unfortunately, it does **not** contain a predefined method for emoji, so we will have to use the `emo` package for
removing the emoji and come up with our own method for extracting them.

---
# Extracting Emoji (2)

First we want to replace the emoji with a textual description, so that we can treat it just like any other token in text mining. This is no trivial task, as we have to go through each comment and replace each emoji with its respective textual description. Unfortunately, we did not find a working, easy to use, out of the box solution for this. But we can always make our own!

Essentially, we want to replace this:


```
## üòÑ
```

with this


```
## [1] "EMOJI_GrinningFaceWithSmilingEyes"
```

---
# Extracting Emoji (3)

First of all, we need a dataframe that contains the emoji as they are internally represented by `R` (this can be quite the [hassle](https://dss.iq.harvard.edu/blog/escaping-character-encoding-hell-r-windows)). Luckily, this is contained in the [`emo` package](https://github.com/hadley/emo).


```r
library(emo)
EmojiList &lt;- jis
EmojiList[1:3,c(1,3,4)]
```

```
## # A tibble: 3 √ó 3
##   runes emoji name                          
##   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;                         
## 1 1F600 üòÄ    grinning face                 
## 2 1F601 üòÅ    beaming face with smiling eyes
## 3 1F602 üòÇ    face with tears of joy
```

---
# Extracting Emoji (4)

Next, we need to paste the names of the emoji together while capitalizing the first letter of every word for better readability


```r
# Defining a function for capitalizing and pasting names together
simpleCap &lt;- function(x) {

  # Splitting the string
  splitted &lt;- strsplit(x, " ")[[1]]

  # Pasting it back together with capital letters
  paste(toupper(substring(splitted, 1,1)),
        substring(splitted, 2),
        sep = "",
        collapse = " ")
}
```

---
# Extracting Emoji (5)


```r
# Applying the function to all the names
CamelCaseEmojis &lt;- lapply(jis$name, simpleCap)
CollapsedEmojis &lt;- lapply(CamelCaseEmojis,
                          function(x){gsub(" ",
                                           "",
                                           x,
                                           fixed = TRUE)})
EmojiList[,4] &lt;- unlist(CollapsedEmojis)
EmojiList[1:3,c(1,3,4)]
```

```
## # A tibble: 3 √ó 3
##   runes emoji name                      
##   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;                     
## 1 1F600 üòÄ    GrinningFace              
## 2 1F601 üòÅ    BeamingFaceWithSmilingEyes
## 3 1F602 üòÇ    FaceWithTearsOfJoy
```


---
# Extracting Emoji (6)

After that, we need to order our dictionary from longest to shortest, so that we can prevent partial matching of shorter strings later


```r
EmojiList &lt;- EmojiList[rev(order(nchar(jis$emoji))),]
head(EmojiList[,c(1,3,4)],5)
```

```
## # A tibble: 5 √ó 3
##   runes                                      emoji name            
##   &lt;chr&gt;                                      &lt;chr&gt; &lt;chr&gt;           
## 1 1F469 200D 2764 FE0F 200D 1F48B 200D 1F469 üë©‚Äç‚ù§Ô∏è‚Äçüíã‚Äçüë©    Kiss:Woman,Woman
## 2 1F468 200D 2764 FE0F 200D 1F48B 200D 1F468 üë®‚Äç‚ù§Ô∏è‚Äçüíã‚Äçüë®    Kiss:Man,Man    
## 3 1F469 200D 2764 FE0F 200D 1F48B 200D 1F468 üë©‚Äç‚ù§Ô∏è‚Äçüíã‚Äçüë®    Kiss:Woman,Man  
## 4 1F3F4 E0067 E0062 E0077 E006C E0073 E007F  üè¥Û†ÅßÛ†Å¢Û†Å∑Û†Å¨Û†Å≥Û†Åø    Wales           
## 5 1F3F4 E0067 E0062 E0073 E0063 E0074 E007F  üè¥Û†ÅßÛ†Å¢Û†Å≥Û†Å£Û†Å¥Û†Åø    Scotland
```
Note that what we are ordering by the `emoji` column, not the `text` or `runes` columns

---
# Extracting Emoji (7)

Now we can `loop` through all of our emoji and replace them one after the other in each comment (*note*: this may take a while)


```r
# Assigning the column to a TextEmoRep variable
TextEmoRep &lt;- LinkDel

# Looping through all Emojis for all comments in LinkDel
for (i in 1:dim(EmojiList)[1]) {

  TextEmoRep &lt;- rm_default(TextEmoRep,
                  pattern = EmojiList[i,3],
                  replacement = paste0("EMOJI_",
                                       EmojiList[i,4],
                                       " "),
                  fixed = TRUE,
                  clean = FALSE,
                  trim = FALSE)
}
```

---
# Extracting Emoji (8)

As output, we get a large character vector with replaced emoji


```r
Selection$textOriginal[233]
```

```
## [1] "Current like to dislike ratio:\nüëç47K  üëé167K"
```

```r
TextEmoRep[233]
```


```
## [1] "Current like to dislike ratio"            
## [2] " EMOJI_ThumbsUp 47K EMOJI_ThumbsDown 167K"
```

---
# Extracting Emoji (9)

.ssmall[

```r
ExtractEmoji &lt;- function(x){

  SpacerInsert &lt;- gsub(" ","[{[SpAC0R]}]", x)
  ExtractEmoji &lt;- rm_between(SpacerInsert,
                             "EMOJI_","[{[SpAC0R]}]",
                             fixed = TRUE,
                             extract = TRUE,
                             clean = FALSE,
                             trim = FALSE,
                             include.markers = TRUE)
  
  UnlistEmoji &lt;- unlist(ExtractEmoji)
  DeleteSpacer &lt;- sapply(UnlistEmoji,
                         function(x){gsub("[{[SpAC0R]}]",
                                          " ",
                                          x,
                                          fixed = TRUE)})
  
  names(DeleteSpacer) &lt;- NULL
  Emoji &lt;- paste0(DeleteSpacer, collapse = "")
  return(Emoji)
}
```
]

---
# Extracting Emoji (10)

We can apply the function to get one vector containing only the emoji as textual descriptions


```r
Emoji &lt;- sapply(TextEmoRep,ExtractEmoji)
names(Emoji) &lt;- NULL
LinkDel[233]
```

```
## [1] "Current like to dislike ratio: üëç47K üëé167K"
```

```r
Emoji[233]
```

```
## [1] "EMOJI_ThumbsUp EMOJI_ThumbsDown "
```

---
# Removing Emoji

In addition, we remove the emoji from our `LinkDel` variable to have one _clean_ column that we can use for text mining later. This column will not contain hyperlinks or emoji.


```r
# We take the LinkDel column and also delete the emoji from it
library(emo)
LinkDel[233]
```

```
## [1] "Current like to dislike ratio: üëç47K üëé167K"
```

```r
TextEmoDel &lt;- ji_replace_all(LinkDel,"")
TextEmoDel[233]
```

```
## [1] "Current like to dislike ratio: 47K 167K"
```

---
# Extracting Information

We now have different versions of our text column

1) The original one, with hyperlinks and emoji (`Selection$textOriginal`)

2) One with only plain text and without hyperlinks and emoji (`TextEmoDel`)

3) One with only hyperlinks (`Links`)

4) One with only emoji (`Emoji`)

We want to integrate all of them in our dataframe.

---
# Linking everything back together

We can now combine our dataframe with the additional columns we created to have the perfect starting point for our analysis! However, because we sometimes have more than two links or two emoji per comment, we need to use the `I()` function so we can put them in the dataframe `as is`. Later, we will have to unlist these columns rowwise if we want to use them. 


```r
df &lt;- cbind.data.frame(Selection$authorDisplayName,
                       Selection$textOriginal,
                       TextEmoRep,
                       TextEmoDel,
                       Emoji = I(Emoji),
                       Selection$likeCount,
                       Links = I(Links),
                       Selection$publishedAt,
                       Selection$updatedAt,
                       Selection$parentId,
                       Selection$id,
                       stringsAsFactors = FALSE)
```
---
# Linking everything back together

As a final step, we can give the columns appropriate names and save the dataframe for later use


```r
# setting column names
names(df) &lt;- c("Author",
               "Text",
               "TextEmojiReplaced",
               "TextEmojiDeleted",
               "Emoji",
               "LikeCount",
               "URL",
               "Published",
               "Updated",
               "ParentId",
               "CommentID")

saveRDS(df, file = "../../data/ParsedEmojiComments.rds")
```

---

class: center, middle

# [Exercise](https://jobreu.github.io/youtube-workshop-gesis-2021/exercises/A4_Preprocessing_and_cleaning_data_question.html) time üèãÔ∏è‚Äç‚ôÄÔ∏èüí™üèÉüö¥

## [Solutions](https://jobreu.github.io/youtube-workshop-gesis-2021/solutions/A4_Preprocessing_and_cleaning_data_solution.html)
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
